# Takeaways

In this level, we exploit the `unlink` macro that is used in the glibc's code when `free` is invoked.

## Top chunk (The wilderness)

---

From the [source code](https://elixir.bootlin.com/glibc/glibc-2.11.2/source/malloc/malloc.c#L2218):
> The top-most available chunk (i.e., the one bordering the end of available memory) is treated specially. It is never included in any bin, is used only if no other chunk is available, and is released back to the system if it is very large (see M_TRIM_THRESHOLD). [...]

The top (wilderness) has the same layout as a chunk, thus we can view the heap's remaining size by examining the top's content

## Internal structures

---

```C
struct malloc_chunk {

  INTERNAL_SIZE_T      prev_size;  /* Size of previous chunk (if previous free).  */
  INTERNAL_SIZE_T      size;       /* Size in bytes, including overhead. */

  struct malloc_chunk* fd;         /* double links -- used only if free. */
  struct malloc_chunk* bk;

  /* Only used for large blocks: pointer to next larger size.  */
  struct malloc_chunk* fd_nextsize; /* double links -- used only if free. */
  struct malloc_chunk* bk_nextsize;
};
typedef struct malloc_chunk* mchunkptr;
typedef struct malloc_chunk* mbinptr;
typedef struct malloc_chunk* mfastbinptr;

/* Conveniently, the unsorted bin can be used as dummy top on first call */
#define initial_top(M)              (unsorted_chunks(M))
struct malloc_state {
  /* Base of the topmost chunk -- not otherwise kept in a bin */
  mchunkptr        top;
  ...
};

static struct malloc_state main_arena;
```

## `free()` API

---

```C
void free (void *mem) {
3694:  mstate ar_ptr;
3695:  mchunkptr p;                /* chunk corresponding to mem */
...
3707:  p = mem2chunk(mem);
...
3725:  ar_ptr = arena_for_chunk(p);
...
3739:  _int_free(ar_ptr, p);
}
```

## `free()`'s (exploitable) implementation

---

```C
_int_free(mstate av, mchunkptr p) {
// These two variablues are used by the unlink macro as temps.
4772:  mchunkptr       bck;         /* misc temp for linking */
4773:  mchunkptr       fwd;         /* misc temp for linking */

//We control size since it is part of the chunk
4780:  size = chunksize(p);

4812:  if ((unsigned long)(size) <= (unsigned long)(get_max_fast ()) {
         ...
4885:  }
4886:
4887:  /*
4888:  Consolidate other non-mmapped chunks as they arrive.
4889:  */
4890:  
4891:  else if (!chunk_is_mmapped(p)) {

// We obviously control p
// We control size. Thus we control the variable nextchunk
4908:    nextchunk = chunk_at_offset(p, size);

//We control nextchunk, thus nextsize
4932:    nextsize = chunksize(nextchunk);

//Effectively we remove p from the bin list
4943:    /* consolidate backward */
4944:    if (!prev_inuse(p)) {
4945:      prevsize = p->prev_size;
4946:      size += prevsize;
4947:      p = chunk_at_offset(p, -((long) prevsize));
4948:      unlink(p, bck, fwd);   // !Potentially exploitable!
4949:    }
4950:
4951:    if (nextchunk != av->top) {
4952:      /* get and clear inuse bit */
//We control both nextchunk and nextsize ==> we control nextinuse
4953:      nextinuse = inuse_bit_at_offset(nextchunk, nextsize);
4954:
4955:      /* consolidate forward */
4956:      if (!nextinuse) {
4957:        unlink(nextchunk, bck, fwd); // !Potentially exploitable!
4958:        size += nextsize;
4959:      } else {
4960:        clear_inuse_bit_at_offset(nextchunk, 0);
           }
4961:
4962:      /*
4963:      Place the chunk in unsorted chunk list. Chunks are
4964:      not placed into regular bins until after they have
4965:      been given one chance to be used in malloc.
4966:      */
4967:

4977:      if (!in_smallbin_range(size))
4978:       {
4979:         p->fd_nextsize = NULL;
4980:         p->bk_nextsize = NULL;
4981:       }

4985:      set_head(p, size | PREV_INUSE);
4986:      set_foot(p, size);

4989:    }
4996:    else {
           /*
           If the chunk borders the current high end of memory,
           consolidate into top
           */
4997:      size += nextsize;
4998:      set_head(p, size | PREV_INUSE);
4999:      av->top = p;
5000:      check_chunk(av, p);
5001:    }
5042:  }
...
}
```

## macro reference

---

```C
// ~~~~~ malloc/malloc.c ~~~~~ //

/* conversion from malloc headers to user pointers, and back */
#define SIZE_SZ        4
#define chunk2mem(p)   ((Void_t*)((char*)(p) + 2*SIZE_SZ))
#define mem2chunk(mem) ((mchunkptr)((char*)(mem) - 2*SIZE_SZ))



/* size field is or'ed with PREV_INUSE when previous adjacent chunk in use */
#define PREV_INUSE 0x1

/* extract inuse bit of previous chunk */
#define prev_inuse(p)       ((p)->size & PREV_INUSE)


/* size field is or'ed with IS_MMAPPED if the chunk was obtained with mmap() */
#define IS_MMAPPED 0x2

/* check for mmap()'ed chunk */
#define chunk_is_mmapped(p) ((p)->size & IS_MMAPPED)


/* size field is or'ed with NON_MAIN_ARENA if the chunk was obtained
   from a non-main arena.  This is only set immediately before handing
   the chunk to the user, if necessary.  */
#define NON_MAIN_ARENA 0x4

/* check for chunk from non-main arena */
#define chunk_non_main_arena(p) ((p)->size & NON_MAIN_ARENA)

/*
  Bits to mask off when extracting size

  Note: IS_MMAPPED is intentionally not masked off from size field in
  macros for which mmapped chunks should never be seen. This should
  cause helpful core dumps to occur if it is tried by accident by
  people extending or adapting this malloc.
*/
#define SIZE_BITS (PREV_INUSE|IS_MMAPPED|NON_MAIN_ARENA)

/* Get size, ignoring use bits */
#define chunksize(p)         ((p)->size & ~(SIZE_BITS))

/* Ptr to next physical malloc_chunk. */
#define next_chunk(p) ((mchunkptr)( ((char*)(p)) + ((p)->size & ~SIZE_BITS) ))

/* Ptr to previous physical malloc_chunk */
#define prev_chunk(p) ((mchunkptr)( ((char*)(p)) - ((p)->prev_size) ))

/* Treat space at ptr + offset as a chunk */
#define chunk_at_offset(p, s)  ((mchunkptr)(((char*)(p)) + (s)))

/* check inuse bits in known places */
#define inuse_bit_at_offset(p, s)\
  (((mchunkptr)(((char*)(p)) + (s)))->size & PREV_INUSE)

/* Set size/use field */
#define set_head(p, s)   ((p)->size = (s))

/* Set size at footer (only when chunk is not in use) */
#define set_foot(p, s)   (((mchunkptr)((char*)(p) + (s)))->prev_size = (s))

/* Take a chunk off a bin list */
#define unlink(P, BK, FD) {                                            \
  FD = P->fd;                                                          \
  BK = P->bk;                                                          \
  if (corrupted double-linked list) {                                  \
    /* ... */                                                          \
  } else {                                                             \
    FD->bk = BK;                                                       \
    BK->fd = FD;                                                       \
    if (!in_smallbin_range (P->size)                                   \
        && /* .. */) {                                                 \
        /* ... */                                                      \
    }                                                                  \
  }                                                                    \
}


// ~~~~~ malloc/arena.c ~~~~~ //
#define arena_for_chunk(ptr) \
    (chunk_non_main_arena(ptr) ? heap_for_ptr(ptr)->ar_ptr : &main_arena)

```
