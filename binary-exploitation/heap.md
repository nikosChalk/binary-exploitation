
<link rel="stylesheet" href="./style.css">

<br/>

## Preliminary

---

For in-depth write up of how the heap works checkout these two articles:

1. <https://azeria-labs.com/heap-exploitation-part-1-understanding-the-glibc-heap-implementation/>
2. <https://azeria-labs.com/heap-exploitation-part-2-glibc-heap-free-bins/>

For a guide to developing heap exploits, checkout this article:<br/>
<https://azeria-labs.com/heap-exploitation-part-1-understanding-the-glibc-heap-implementation/>

Another well-written article for understanding glibc malloc:<br/>
https://sploitfun.wordpress.com/2015/02/10/understanding-glibc-malloc/

<br/>

## Introduction

---

When developers are using various stack-based exploit mitigations, attackers often instead build their exploits using heap-related vulnerabilities such as <u>use-after-frees</u>, <u>double-frees</u>, and <u>heap-overflows</u>.<br/>
Heap-based vulnerabilities are more difficult to understand than their stack-based counterparts because attack techniques against heap-based vulnerabilities can be very dependent on how the internal implementation of the heap allocator actually works.

The way the heap works is very platform and implementation specific; lots of different heap implementations exist. For example, Google Chrome’s <i>[PartitionAlloc](https://chromium.googlesource.com/chromium/src/+/HEAD/base/allocator/partition_allocator/PartitionAlloc.md)</i> is very different to the [jemalloc](https://github.com/jemalloc/jemalloc/wiki/Background) heap allocator used in FreeBSD. The default [glibc heap implementation](https://sourceware.org/glibc/wiki/MallocInternals) in Linux is also very different to how the heap works in Windows.

In this tutorial we will focus on the glibc heap implementation. This heap is derived from the [ptmalloc](http://www.malloc.de/en/) heap implementation, which is itself derived from the much older [dlmalloc](http://gee.cs.oswego.edu/dl/html/malloc.html) (Doug Lea malloc) memory allocator.

## `mmap` syscall

---

`void* mmap(void* addr, size_t length, int prot, int flags, int fd, off_t offset);`
Directly from the manpage of `mmap`:
> `mmap()` creates a new mapping in the virtual address space of the calling process.

In other words, `mmap` allows us to allocate new pages for the calling process and specify their permissions (rwx) and type (e.g. shared or private). Internally, it invokes the `mmap` or `mmap2` syscall.

## `brk` syscall

---

Directly from the manpage of `brk`:
> `brk()` and `sbrk()` change the location of the program break, which defines the end of the process's data segment (i.e., the program break is the first location after the end of the uninitialized data segment). Increasing the program break has the effect of allocating memory to the process; decreasing the break deallocates memory. 

Internally, it invokes the `brk` syscall.

## `mmap` vs `mmap2`

---

Signatures are the same.

```C
void* mmap(void* addr, size_t length, int prot, int flags, int fd, off_t offset);
void* mmap2(void* addr, size_t length, int prot, int flags, int fd, off_t pgoffset);
```

Directly from the manpage of `mmap2`:
> The `mmap2()` system call provides the same interface as `mmap(2)`, except that the final argument specifies the offset into the file in 4096-byte units (instead of bytes, as is done by `mmap(2)`). This enables applications that use a 32-bit off_t to map large files (up to 2^44 bytes).

## `mmap` vs `brk`

---

`brk()` is a traditional way of allocating memory in UNIX -- it just expands the data area by a given amount. On the other hand, `mmap()` allows you to allocate independent regions of memory without being restricted to a single contiguous chunk of virtual address space.


## How the Heap works?

---

<u><i>Memory Chunks</i></u>.<br/>
<i>Memory Chunks</i> are areas of memory which have been allocated through the heap allocator and contain both metadata for the heap allocator and the user's data. For example, when a user invokes `malloc(8)`, a chunk is allocated which will contain 8 Bytes for the user and some additional Bytes that will be used for the metadata.<br/>
<img class="center" src="./heap-images/chunk.png"><br/>
The simplified chunk-allocation strategy for small chunks is this:

1. If there is a previously-freed chunk of memory, and that chunk is big enough to service the request, the heap manager will use that freed chunk for the new allocation.
2. Otherwise, if there is available space at the top of the heap, the heap manager will allocate a new chunk out of that available space and use that.
3. Otherwise, the heap manager will ask the kernel to add new memory to the end of the heap, and then allocates a new chunk from this newly allocated space.
4. If all these strategies fail, the allocation can’t be serviced, and malloc returns NULL.

Very large allocation requests (128KB up to 512KB on 32-bit systems and 32MB on 64-bit systems, however this threshold can also dynamically increase) get special treatment in the heap manager. These large chunks are allocated off-heap using a direct call to `mmap`. When these huge allocations are later returned to the heap manager via a call to free, the heap manager releases the entire mmaped region back to the system via `munmap`.

<u><b>Finally</b></u>, the heap manager also needs to ensure that the allocation will be 8-byte aligned on 32-bit systems, or 16-byte aligned on 64-bit systems.

<u><i>Bins</i></u>.<br/>
The heap manager tracks the freed chunks in a series of different linked lists called <i>bins</i>. When an allocation request is made, the heap manager searches those bins for a free chunk that is big enough to service the request. If it finds one, it removes that chunk from the bin, mark it as “allocated”, and then return a pointer to the “user data” region of that chunk to the programmer as the return value of malloc.

<u><i>Arenas</i></u>.<br/>
In multithreaded application we want to avoid threads to bottle-neck in the memory allocator, which is used heavily them, but also avoid race conditions. This need introduces the concept of <i>Arenas</i>.<br/>
Each <i>Arena</i> is essentially an entirely different heap that manages its own chunk allocation and free bins completely separately. For each new thread that joins the process, the heap manager tries to find an arena that no other thread is using and attaches the arena to that thread.

The initial (“main”) arena of the program only contains the heap we’ve already seen, and for single-threaded applications this is the only arena that the heap manager will ever use. The heap is expanded and shrunk through the `brk` syscall.

Secondary arenas emulate the behavior of the main heap using one or more <i>subheaps</i> created using `mmap` and `mprotect`.

<u><i>Subheaps</i></u>.<br/>
<i>Subheaps</i> are an emulation of an actual heap. Their differences are:

1. The original heap is located immediately after where the program is loaded into memory
2. The original heap is dynamically expanded by `brk` syscall. By contrast, each <i>subheap</i> is positioned into memory using `mmap`, and the heap manager manually emulates growing the subheap using `mprotect`.

By default, the maximum size of a subheap is 1MB on 32-bit processes and 64MB on 64-bit systems.
<img class="center" src="./heap-images/memory-layout.png"><br/>

### Chunk Metadata

The exact layout of the chunk metadata in memory can be a bit confusing, because the heap manager source code combines metadata at the end of one chunk with the metadata at the start of the next, and several of the metadata fields exist or are used depending on various characteristics of the chunk (e.g. if it is freed or not).

The chunk's layout for <i>live allocations</i> (allocations that are still being used by the user) can be seen bellow:
<img class="center" src="./heap-images/chunk-live-layout.png"><br/>
Notes:

1. `chunk size` is 8-byte (or 16-byte on 64-bit) aligned thus the last 3 bits are free to use for other purposes, e.g. as the A,M,P bits.
2. `chunk size` includes in its size count both the user data and the chunk's metadata.

Check the `struct malloc_chunk` in malloc/malloc.c glibc's source code for more in-depth and up-to-date information.

## Heap Internals

---

### How `free` works

`free` first does a couple of basic sanity checks to see if the freed pointer is invalid before attempting to process it. If any of the checks fail, the program aborts. The checks include:

1. A check that the allocation is aligned on an 8-byte (or 16-byte on 64-bit) boundary, since malloc ensures all allocations are aligned.
2. A check that the chunk’s size field isn’t impossible – either because it is too small, too large, not an aligned size, or would overlap the end of the process’ address space.
3. A check the chunk lies within the boundaries of the arena.
4. A check that the chunk is not already marked as free by checking the corresponding `P` bit that lies in the metadata at the start of the next chunk.

Afterwards, `free` figures out to which memory chunk does the pointer belong to and performs the following algorithm:

1. If the chunk has the `M` bit set in the metadata, the allocation was allocated off-heap and should be `munmaped`.
2. Otherwise, if the chunk before this one is free, the chunk is merged backwards to create a bigger free chunk.
3. Similarly, if the chunk after this one is free, the chunk is merged forwards to create a bigger free chunk.
4. If this potentially-larger chunk borders the “top” of the heap, the whole chunk is absorbed into the end of the heap, rather than stored in a bin.
5. Otherwise, the chunk is marked as free and placed in an appropriate bin.

### Freed Chunks

The freed chunks are stored in corresponding “free bins” that operate as linked lists. This requires each free chunk to also store pointers to other chunks. Since the “user data” in the freed chunk is (by definition) free for use by the heap manager, the heap manager repurposes this “user data” region in freed chunks as the place where this additional metadata lives.
<img class="center" src="./heap-images/chunk-free-layout.png"><br/>

### Types of Bins

<u>There are 5 type of bins</u>: <br/>
62 <u><i>small bins</i></u>, 63 <u><i>large bins</i></u>, 1 <u><i>unsorted bin</i></u>, 10 <u><i>fast bins</i></u> and 64 <u><i>tcache bins per thread</i></u>.<br/>
Confusingly, the <i>small</i>, <i>large</i>, and <i>unsorted</i> bins all live together in the same array in the heap manager’s [source code](https://sourceware.org/git/gitweb.cgi?p=glibc.git;a=blob;f=malloc/malloc.c;h=6e766d11bc85b6480fa5c9f2a76559f8acf9deb5;hb=HEAD#l1686). 

```C
// Fields are defined in struct malloc_state
bins[0] = unused
bins[1] = unsorted bin
[bins[2], bins[63]] = small bins
[bins[64], bins[126]] = large bins

fastbinsY = fast bins
```

<b>One key note</b> is that, given a chunk’s size, there is exactly one small bin or large bin that this size corresponds to.

#### Small Bins

<img class="center" src="./heap-images/small-bins.png"><br/>

#### Large Bins

<img class="center" src="./heap-images/large-bins.png"><br/>

#### Unsorted Bin

The heap manager improves this basic algorithm one step further using an optimizing cache layer called the <i>unsorted bin</i>.  Instead of immediately putting newly freed chunks onto the correct bin, the heap manager coalesces it with neighbors, and dumps it onto a general unsorted linked list. During malloc, each item on the unsorted bin is checked to see if it “fits” the request. If it does, malloc can use it immediately. If it does not, malloc then puts the chunk into its corresponding small or large bin.
<img style="margin: 10px 0 0 40px" src="./heap-images/unsorted-bin.png" height="500"><br/>

#### Fast Bins

<i>Fast bins</i> are a further optimization layered on top of the three basic bins we’ve already seen. These bins essentially keep recently released small chunks on a single linked list with the LIFO property. Double linking is not necessary and ordering inside a bin doesn't matter. Unlike small bins, fast bins chunks are never merged with their neighbors and the `P` bit at the start of the next chunk is never set.

On the other hand, like small bins, each fast bin is responsible only for a single fixed chunk size and there are 10 such fast bins.

The downside of fast bins, is that fastbin chunks are not “truly” freed or merged, and this would eventually cause the memory of the process to fragment and balloon over time. To resolve this, heap manager periodically consolidates fastbins and placing the resulting free chunks onto the unsorted bin for malloc to later use. This consolidation stage occurs whenever a malloc request is made that is larger than a fastbin can service (i.e. [chunks over 512 bytes on 32-bit or 1024 bytes on 64-bit](https://sourceware.org/git/gitweb.cgi?p=glibc.git;a=blob;f=malloc/malloc.c;h=6e766d11bc85b6480fa5c9f2a76559f8acf9deb5;hb=HEAD#l3696), [freeing any chunk over 64KB](https://sourceware.org/git/gitweb.cgi?p=glibc.git;a=blob;f=malloc/malloc.c;h=6e766d11bc85b6480fa5c9f2a76559f8acf9deb5;hb=HEAD#l3696), calling [mallocopt](http://man7.org/linux/man-pages/man3/mallopt.3.html) or [malloc_trim](http://man7.org/linux/man-pages/man3/malloc_trim.3.html))

<img src="./heap-images/fast-bins.png" height="500"><br/>

#### Tcache Bins

TODO
